{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparatory work\n",
    "\n",
    "We first load the necessary backend and the necessary library for plot to work correctly and to to our computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn import mixture\n",
    "from scipy import stats\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote here a function to read the Amerge.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(letter : str):\n",
    "\n",
    "    path = './Unistroke/'+letter+'merge.txt'\n",
    "    with open(path, 'r') as f:\n",
    "        nb_line = sum(1 for _ in f)\n",
    "    with open(path, 'r') as f:\n",
    "        data = np.zeros((nb_line,2))\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            data[i][0] = float(line.strip().split(\" \")[0])\n",
    "            data[i][1] = float(line.strip().split(\" \")[1])\n",
    "            i+=1\n",
    "        return data\n",
    "\n",
    "def read_and_merge(letter : str):\n",
    "    files = os.listdir('./Unistroke/')\n",
    "    regex = re.compile(r'^'+letter+'[0-9]')\n",
    "    files = list(filter(regex.search, files))\n",
    "    data = list()\n",
    "    for file in files:\n",
    "        with open(\"./Unistroke/\"+file) as content: \n",
    "            for line in content:\n",
    "                data.append((float(line.strip().split(\"\\t\")[0]),float(line.strip().split(\"\\t\")[1])))\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{1. Simulate a sample of size 500 of the following bivariate GMM :}$\n",
    "\n",
    "$$ 0.3 N( \\mu_1,\\Sigma_1)+ 0.7 N(\\mu_2,\\Sigma_2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [-3,0]\n",
    "cov1 = [[5,-2],[-2,1]]\n",
    "\n",
    "mean2 = [3,0]\n",
    "cov2 = [[5,2],[2,2]]\n",
    "\n",
    "d1 = np.random.multivariate_normal(mean1, cov1, 500)\n",
    "d2 = np.random.multivariate_normal(mean2, cov2, 500)\n",
    "\n",
    "mixt = np.concatenate((d1,d2))\n",
    "\n",
    "\n",
    "plt.plot(mixt[:,0], mixt[:,1],\".\")\n",
    "plt.title('Simulated gaussian mixture')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the simulated gaussian mixture looks like the figures in the slides of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis: Gaussian model\n",
    "\n",
    "In this section, we analyse data coming from the Unistroke data set. \n",
    "We would like to know if a bivariate Gaussian mixture can make for a good representation of the letter A.\n",
    "\n",
    "We use the mixture package from the library sklearn to avoid implementing ourselves the EM algorithm for estimating parameters of a Gaussian mixture.\n",
    "\n",
    "$\\textbf{1. Estimate a bivariate GMM on the letter A data set and provide the estimated parameters.}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We first read and plot the letter A data set after angular transformation. The transformation is the following :\n",
    "  \n",
    "  $$ x'_n = \\frac{x_{n+1}-x_n}{|x_{n+1}-x_n|}$$\n",
    "  \n",
    "  We understand why the data points appear on a circle using this transformation as $|x'_n|=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"A\")\n",
    "plt.figure()\n",
    "plt.plot(data[:,0], data[:,1],'.')\n",
    "plt.title('Merging of all A files after angular transformation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the estimation we use the function fit() from the gaussian\\_mixture\n",
    "package. The only parameter we define is the number of components of the Gaussian mixture, here n\\_components=2. The algorithm fits the model using an EM algorithm for Gaussian mixtures.\n",
    "The stopping criterion is reached when the change of likelihood between two iteration is less than $\\textbf{tol}$. The maximum number of iteration is $\\textbf{max_iter}$. The initialisation process is assured internally by the method \\_initialize\\_parameters(self , X, random\\_state). We can see in this method that the initialisation process can be done using the k-mean algorithm or using random initial parameters. by default it will use a k-mean algorithm.\n",
    "We chose to let every parameters set as the default ones.\n",
    "We chose here to let the full freedom to the covariance matrices (option full).\n",
    "The algorithm return the following estimated parameters for our data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means :  [[-0.36571287  0.9060224 ]\n",
      " [-0.29510373 -0.72483125]]\n",
      "\n",
      "\n",
      "covs :  [[[0.03903012 0.01426564]\n",
      "  [0.01426564 0.00634938]]\n",
      "\n",
      " [[0.27188755 0.0823439 ]\n",
      "  [0.0823439  0.1156479 ]]]\n"
     ]
    }
   ],
   "source": [
    "#EM algorithm for gaussian mixtures\n",
    "gmm = mixture.GaussianMixture(2,covariance_type='full')\n",
    "gmm.fit(data)\n",
    "print(\"means : \",gmm.means_)\n",
    "print(\"\\n\")\n",
    "print(\"covs : \",gmm.covariances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained are the means and the covariance matrices of each of our 2 estimated Gaussian after the EM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2. Label the data using the estimated model and show the pdf of the estimated GMM}$\n",
    "\n",
    "To label the data we use the method predict from the Gaussian mixture.\n",
    "The predict method use the fitted parameters from the function fit to estimate the probability of each data point to belong to one cluster or the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# meshgrid\n",
    "labels = gmm.predict(data)\n",
    "x = np.linspace(-2,2,500)\n",
    "X,Y = np.meshgrid(x,x)\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X\n",
    "pos[:, :, 1] = Y\n",
    "# pdf\n",
    "rv = stats.multivariate_normal(gmm.means_[0],gmm.covariances_[0])\n",
    "rv2 = stats.multivariate_normal(gmm.means_[1],gmm.covariances_[1])\n",
    "# color with classification\n",
    "bool_label= list(map(bool,labels))\n",
    "inv_bool_label = [not i for i in bool_label]  \n",
    "data1 = data[bool_label]\n",
    "data2 = data[inv_bool_label]\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4),constrained_layout=True)\n",
    "\n",
    "axs[0].contourf(X,Y,rv.pdf(pos))\n",
    "axs[0].plot(data1[:,0], data1[:,1],'.', color='g')\n",
    "axs[0].plot(data2[:,0], data2[:,1],'.', color='b')\n",
    "axs[0].set_title('fitted distribution of blue data points')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].set_ylabel('y')\n",
    "\n",
    "axs[1].contourf(X,Y,rv2.pdf(pos))\n",
    "axs[1].plot(data1[:,0], data1[:,1],'.', color='g')\n",
    "axs[1].plot(data2[:,0], data2[:,1],'.', color='b')\n",
    "axs[1].set_title('fitted distribution of green data points')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "\n",
    "fig.suptitle('classification of data points and \\n gaussian pdf estimated by the EM algorithm',fontsize=15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{3 (a) : Plot each marginal histogram and add the estimated mixture of univariate\n",
    "Gaussian pdfs to the figure.}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(1, 2, figsize=(8, 4),constrained_layout=True)\n",
    "\n",
    "axs2[0].hist(data[:,0],150,  density = True)\n",
    "axs2[1].hist(data[:,1],150,  density = True)\n",
    "\n",
    "mg = stats.norm(gmm.means_[0,0], gmm.covariances_[0,0,0])\n",
    "mg2 = stats.norm(gmm.means_[0,1], gmm.covariances_[0,1,1])\n",
    "mg3 = stats.norm(gmm.means_[1,0], gmm.covariances_[1,0,0])\n",
    "mg4 = stats.norm(gmm.means_[1,1], gmm.covariances_[1,1,1])\n",
    "\n",
    "axs2[0].plot(x,(mg.pdf(x)+mg3.pdf(x))*0.5)\n",
    "axs2[1].plot(x,(mg2.pdf(x)+mg4.pdf(x))*0.5)\n",
    "axs2[0].set_title('marginal along x')\n",
    "axs2[1].set_title('marginal along y')\n",
    "\n",
    "axs2[0].set_xlabel('x')\n",
    "axs2[0].set_ylabel('density')\n",
    "axs2[1].set_xlabel('y')\n",
    "axs2[1].set_ylabel('density')\n",
    "\n",
    "fig2.suptitle('marginal histograms and marginal \\n estimated gaussian mixtures',fontsize=15)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf{3. (b) For each marginal, provide separate histograms of each cluster and add the estimated\n",
    " Gaussian pdf to the figure.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1e326338ad8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mfig3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'marginal estimated pdf and histogram for each cluster '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfig3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'cls'"
     ]
    }
   ],
   "source": [
    "fig3, axs3 = plt.subplots(2, 2, figsize=(8, 4),constrained_layout=True)\n",
    "bar_num = 40\n",
    "axs3[0][0].hist(data1[:,0],bar_num,  density = True,color='g')\n",
    "axs3[0][0].plot(x,(mg3.pdf(x)),color='r')\n",
    "axs3[0][0].set_title('marginal along x, green cluster')\n",
    "\n",
    "axs3[0][1].hist(data1[:,1], bar_num,  density = True,color='g')\n",
    "axs3[0][1].plot(x,(mg4.pdf(x)),color='r')\n",
    "axs3[0][1].set_title('marginal along y, green cluster')\n",
    "\n",
    "axs3[1][0].hist(data2[:,0],bar_num,  density = True,color='b')\n",
    "axs3[1][0].plot(x,(mg.pdf(x)),color='r')\n",
    "axs3[1][0].set_title('marginal along x, blue cluster')\n",
    "\n",
    "axs3[1][1].hist(data2[:,1],bar_num,  density = True,color='b')\n",
    "axs3[1][1].plot(x,(mg2.pdf(x)),color='r')\n",
    "axs3[1][1].set_title('marginal along y, blue cluster')\n",
    "\n",
    "axs3[0][0].set_xlabel('x')\n",
    "axs3[0][0].set_ylabel('density')\n",
    "axs3[1][0].set_xlabel('x')\n",
    "axs3[1][0].set_ylabel('density')\n",
    "axs3[0][1].set_xlabel('y')\n",
    "axs3[0][1].set_ylabel('density')\n",
    "axs3[1][1].set_xlabel('y')\n",
    "axs3[1][1].set_ylabel('density')\n",
    "\n",
    "\n",
    "fig3.suptitle('marginal estimated pdf and histogram for each cluster ',fontsize=15)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Plot each data point x i with some colourmap corresponding to}$ $P(Z_i = 1|X_i )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_proba=gmm.predict_proba(data)\n",
    "plt.scatter(data[:,0], data[:,1], c=np.log(posterior_proba[:,0]))\n",
    "plt.title('data points given posterior probability')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $P(Z_i = 1|X_i )$, we use the predict_proba() function. It returns for each fitted gaussian the probability of each data points to be generated by the said Gaussian. This can be interpreted as a \"soft\" or continuous labelling of the data points. We see on the plot above a clear dichotomy between two sets of data points corresponding to the two different labelled clusters. This was the expected results. Using the log of the posterior probability helps for visualisation as the transition between the two cluster is too \"sharp\" without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_proba=gmm.predict_proba(data)\n",
    "plt.scatter(data[:,0], data[:,1], c=posterior_proba[:,0])\n",
    "plt.title('data points given posterior probability')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
